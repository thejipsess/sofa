shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0)))
label_1 <- "Death"
label_0 <- "Alive"
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0)))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0) ~ %<->%))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0)  %<->%))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0) ~ "%<->%"))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0) ~ %<=>%))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0)%<=>%))
shap.plot.summary(shap_long_high) + ylab(bquote(%<=>%)
shap.plot.summary(shap_long_high) + ylab(bquote(%<=>%))
shap.plot.summary(shap_long_high) + ylab(bquote(~%<=>%))
shap.plot.summary(shap_long_high) + ylab(bquote(*%<=>%))
shap.plot.summary(shap_long_high) + ylab(bquote(~%<=>%~))
shap.plot.summary(shap_long_high) + ylab(bquote(~<=>~))
shap.plot.summary(shap_long_high) + ylab(bquote(x %<->% y))
shap.plot.summary(shap_long_high) + ylab(bquote(.label_0 %<->% y))
shap.plot.summary(shap_long_high) + ylab(bquote(.(label_0) %<->% y))
shap.plot.summary(shap_long_high) + ylab(bquote(.(label_0) %<->% .(label_1)))
shap.plot.summary(shap_long_high) + ylab(bquote(More likely .(label_0) %<->% .(label_1)))
shap.plot.summary(shap_long_high) + ylab(bquote(More likely ~.(label_0) %<->% .(label_1)))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" .(label_0) %<->% .(label_1)))
shap.plot.summary(shap_long_high) + ylab(bquote("More likely" ~ .(label_0) %<->% .(label_1)))
shap.plot.summary(shap_long_high) + ylab(bquote("Higher predicted probability towards" ~ .(label_0) %<->% ~ "Higher predicted probability towards"~ .(label_1)))
#' @param save Boolean indicating whether to save the plot locally
#' @param save_name String specifying the name of the file to save the plot to
#' if save=TRUE.
#' @param mod If the model has already been trained you can supply it here
#'
#' @return list containing the shapley values, the shapley information in long
#' format, and the ggplot object.
#' @export
#'
#' @examples
plot_shaply_summary <- function(train, mod = NULL, label = NULL, params = NULL,
n_features = 10, save = FALSE,
save_name = "Shapley summary plot"){
# Determine first and second label of dependent variable
label_0 <- names(table(train$y))[1]
label_1 <- names(table(train$y))[2]
if(is.null(params)){
param_list <- list(objective = "binary:logistic",
eval_metric = "logloss")
} else {
param_list <- list(objective = "binary:logistic",
min_child_weight  = params$min_n,
max_depth = params$tree_depth,
eta = params$learn_rate,
gamma = params$loss_reduction,
eval_metric = "logloss")
}
# If dependent variable is present in train, then remove it
if('y' %in% colnames(train)){
label <- train$y
train <- select(train, -y)
}
# Make sure train is a matrix
train <- as.matrix(train)
# Make sure label consists of zeros and ones
if(class(label) != "numeric")
label <- as.numeric(label)
step <- 0
while(max(label) > 1){
label <- label - 1
step <- step + 1
if(step>100) stop("Something seems to be wrong with the supplied label")
}
if(is.null(mod)){
print("Model not supplied so training it now")
mod <- xgboost::xgboost(data = train,
label = as.matrix(label),
params = param_list, nrounds = 100,
verbose = FALSE, nthread = parallel::detectCores())
# To return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod, X_train = train)
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = mod, top_n = n_features,
X_train = train)
} else {
# To return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod$fit, X_train = train)
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = mod$fit, top_n = n_features,
X_train = train)
}
# **SHAP summary plot**
plt <- shap.plot.summary(shap_long) +
ylab(bquote("Higher predicted probability towards" ~ .(label_0) %<->% ~ "Higher predicted probability towards"~ .(label_1)))
if(save)
dev.print(svg, paste("Figures/", save_name, ".svg", sep = ""),
width = 14, height = 7)
return(list(shap_values = shap_values,
shap_long = shap_long,
plt = plt))
}
shap_low <- plot_shaply_summary(bake(sofa_rec, df_low), mod = model_xg, save = F,
n_features = 5)
shap_low$plt <- shap_low$plt + ggtitle("Shapley summary plot - SOFA < 8 on day 1")
shap_mid <- plot_shaply_summary(bake(sofa_rec, df_mid), mod = model_xg, save = F,
n_features = 5)
shap_mid$plt <- shap_mid$plt + ggtitle("Shapley summary plot - SOFA inbetween 8 & 12 on day 1")
shap_high <- plot_shaply_summary(bake(sofa_rec, df_high), mod = model_xg, save = F,
n_features = 5)
shap_high$plt <- shap_high$plt + ggtitle("Shapley summary plot - SOFA > 12 on day 1")
shap_low
shap_mid
shap_high
bake(sofa_rec, df_low)$y
df_low$y
df$y
dp$event
View(df)
View(d)
d$event
View(d)
d$ICU_mortality
View(df)
df_low$y <- factor(df_low, labels = c("Alive", "Death"))
df_low$y
df_low$y <- factor(df_low, levels = c(0,1), labels = c("Alive", "Death"))
df_low$y <- factor(df_low$y, levels = c(0,1), labels = c("Alive", "Death"))
df_mid$y <- factor(df_mid$y, levels = c(0,1), labels = c("Alive", "Death"))
df_high$y <- factor(df_high$y, levels = c(0,1), labels = c("Alive", "Death"))
shap_low <- plot_shaply_summary(bake(sofa_rec, df_low), mod = model_xg, save = F,
n_features = 5)
shap_low$plt <- shap_low$plt + ggtitle("Shapley summary plot - SOFA < 8 on day 1")
shap_mid <- plot_shaply_summary(bake(sofa_rec, df_mid), mod = model_xg, save = F,
n_features = 5)
shap_mid$plt <- shap_mid$plt + ggtitle("Shapley summary plot - SOFA inbetween 8 & 12 on day 1")
shap_high <- plot_shaply_summary(bake(sofa_rec, df_high), mod = model_xg, save = F,
n_features = 5)
shap_high$plt <- shap_high$plt + ggtitle("Shapley summary plot - SOFA > 12 on day 1")
shap_low
shap_mid
shap_high
sofa_rec <- prep(test_rec)
df_low <- bake(sofa_rec, df[df$day1 < 8,])
df_high <- bake(sofa_rec, df[df$day1 > 12,])
df_mid <- bake(sofa_rec, df[df$day >= 8 & df$day1 <=12,])
df_high$y <- factor(df_high$y, levels = c(0,1), labels = c("Alive", "Death"))
df_mid$y <- factor(df_mid$y, levels = c(0,1), labels = c("Alive", "Death"))
df_low$y <- factor(df_low$y, levels = c(0,1), labels = c("Alive", "Death"))
shap_low <- plot_shaply_summary(df_low, mod = model_xg, save = F,
n_features = 5)
shap_low$plt <- shap_low$plt + ggtitle("Shapley summary plot - SOFA < 8 on day 1")
shap_mid <- plot_shaply_summary(df_mid, mod = model_xg, save = F,
n_features = 5)
shap_mid$plt <- shap_mid$plt + ggtitle("Shapley summary plot - SOFA inbetween 8 & 12 on day 1")
shap_high <- plot_shaply_summary(df_high, mod = model_xg, save = F,
n_features = 5)
shap_high$plt <- shap_high$plt + ggtitle("Shapley summary plot - SOFA > 12 on day 1")
shap_low
shap_mid
shap_high
ggsave(shap_low, filename = , save_name = "Shapley summary plot with extende variables - sofa < 8.png")
ggsave(shap_mid, filename = , save_name = "Shapley summary plot with extende variables - sofa inbetween 8 & 12.png")
ggsave(shap_high, filename = , save_name = "Shapley summary plot with extende variables - sofa > 12.png")
ggsave(shap_low, filename = "Figures/Shapley summary plot with extende variables - sofa < 8.png")
ggsave(shap_mid, filename = "Figures/Shapley summary plot with extende variables - sofa inbetween 8 & 12.png")
ggsave(shap_high, filename = "Figures/Shapley summary plot with extende variables - sofa > 12.png")
ggsave(shap_low, filename = "Figures/Shapley summary plot with extende variables - sofa < 8.jpg")
ggsave(shap_low, filename = "Figures/Shapley summary plot with extende variables - sofa < 8.svg")
ggsave(shap_high, filename = "Figures/Shapley summary plot with extende variables - sofa high.png")
shap_low
dev.print(svg, "Figures/Shapley summary plot with extende variables - sofa low.svg",
width = 14, height = 7)
shap_mid
dev.print(svg, "Figures/Shapley summary plot with extende variables - sofa mid.svg",
width = 14, height = 7)
shap_high
dev.print(svg, "Figures/Shapley summary plot with extende variables - sofa high",
width = 14, height = 7)
shap_low
dev.print(png, "Figures/Shapley summary plot with extende variables - sofa low.svg",
width = 14, height = 7)
shap_mid
dev.print(png, "Figures/Shapley summary plot with extende variables - sofa mid.svg",
width = 14, height = 7)
shap_high
dev.print(png, "Figures/Shapley summary plot with extende variables - sofa high",
width = 14, height = 7)
shap_low
shap_mid
shap_high
library(lme4)
library(rms)
library(pROC)
library(data.table)
library(tidyverse)
library(tidymodels)
library(xgboost)
library(themis)
library(tictoc)
library(ggplot2)
library(SHAPforxgboost)
library(fmsb)
library(svglite)
# Load local scripts
source("R/XGBoost.r")
source("R/Parallel.r")
source("R/Visualisation.r")
source("R/FeatureImportance.r")
#==============================================================================#
#
#   This Section contains the code from Sander
#
#==============================================================================#
## Databestand gecreeerd met sofa_data.R inlezen
load("../../Data/sofa_data.Rda")
# Set on what day you want to evaluate the samples
day <- 7
## Persoonslevel regressie om dag 1 en t-dagen verandering sofa score met
## minimale meetfout te bepalen
d <- subset(d, d$dag < day)
# Order samples on ID and day
d <- arrange(d, Record.Id, dag)
models <- lmList(SOFA_score ~ dag | Record.Id, data = d, na.action = na.omit)
coef(models)
res <- data.frame(Record.Id = rownames(coef(models)),
coef(models), check.names = FALSE)
names(res)[2] <- "Intercept"
names(res)[3] <- "Slope"
res$day1  <- round(res$Intercept + 1*res$Slope, 1)
res$day5  <- round(res$Intercept + 5*res$Slope, 1)
res$delta <- round(4*res$Slope, 1)
## Databestand maken waarbij iedere patient één observatie bijdraagt
dp <- d[!duplicated(d$Record.Id, fromLast = TRUE), ]
dp <- merge(dp, res, by = "Record.Id")
empty <- c("X", "X.y", "X.x")
dp <- dp[, !names(dp) %in% empty]
constant <- c("CHC.Dementia", "CHC.Connective_tissue_disease", "CHC.Hemiplegia",
"CHC.AIDS", "ckd_status.Creatinine_265_mmolL", "adrenaline",
"dobutamine", "dopamine", "anti_viral.Isavuconazol", "isOnMediumCare")
dp <- dp[, !names(dp) %in% constant]
## Model om mortaliteit te voorspellen
table(dp$ICU_mortality)
dp$event <- ifelse(dp$ICU_mortality == "Death", 1, 0)
table(dp$event)
## Afgeleiden bepalen
dp$sofa_stijging <- ifelse(dp$delta > 0, 1, 0)
dp$adv_age <- ifelse(dp$age > 65, 1, 0); table(dp$adv_age)
## Selectie?
## dp <- subset(dp, dp$ECMO != 1)
dd <- datadist(dp)
options(datadist = "dd")
## Model enkel op basis van SOFA score dag 1 en delta 1 tot 5
modela <- lrm(event ~ delta, data  = dp, x = TRUE, y = TRUE)
modela
modelb <- lrm(event ~ day1 + delta, data = dp, x = TRUE, y = TRUE)
modelb
## Model aangevuld met geslacht en leeftijd, geslacht discutabel
model2 <-  lrm(event ~ day1 + delta + age, data = dp, x = TRUE, y = TRUE)
model2
model3 <- lrm(event ~ day1 + delta + age + gender, data = dp, x = TRUE, y = TRUE)
model3
## ROC curve
r <- pROC::roc(dp$event, predict(model3, type = "fitted"),  ci = TRUE)
r
png("auc.png", width = 500, height = 500, pointsize = 16)
ggroc(r, legacy.axes = TRUE) +
geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey",
linetype="dashed")
dev.off()
set.seed(070181)
roc_auc <- round(0.5 + 0.5*validate(model2, B = 1000)[1, 5], 2)
roc_auc
## Afkappunten bepalen
construct_decision_matrix(dp, model3) # This function comes from Visualisation.R
## Alternatief: >70 komt er niet in
risk.FN  <- ifelse(dp$age < 75, 0, 1)
sensl <- round(table(risk.FN, dp$event)[2,2]/(table(risk.FN, dp$event)[2,2] +
table(risk.FN, dp$event)[1,2])*100, 1)
specl <- round(table(risk.FN, dp$event)[1,1]/(table(risk.FN, dp$event)[1,1] +
table(risk.FN, dp$event)[2,1])*100, 1)
ppvl  <- round(table(risk.FN, dp$event)[2,2]/(table(risk.FN, dp$event)[2,2] +
table(risk.FN, dp$event)[2,1])*100, 1)
npvl  <- round(table(risk.FN, dp$event)[1,1]/(table(risk.FN, dp$event)[1,1] +
table(risk.FN, dp$event)[1,2])*100, 1)
abstl <- sum(risk.FN == 1, na.rm = TRUE)
miscl <- table(risk.FN, dp$event)[2, 1]
ligdl <- sum(ifelse(dp$ICU_LoS[risk.FN == 1] - 7 < 1, 0,
dp$ICU_LoS[risk.FN == 1] - 7), na.rm = TRUE)
ligpl <- round(ligdl/sum(dp$ICU_LoS, na.rm = TRUE)*100, 1)
afkapl <- "lft>75"
data.frame(afkapl, sensl, specl, ppvl, npvl, abstl, miscl, ligdl, ligpl)
#==============================================================================#
#
#   This Section contains the code from Jip
#
#==============================================================================#
#=======================#
#                       #
#       XGBOOST         #
#                       #
#=======================#
#!!! still need to implement train/test split beforehand
# Set a random seed
seed <- 7649
# Create necessary directories if they do not yet exist
dir.create("Figures", showWarnings = FALSE)
dir.create("Objects", showWarnings = FALSE)
dir.create("Tables", showWarnings = FALSE)
# Prepare date for XGBoost
# df <- dp %>%
#         select(event, day1, delta, age, gender) # select variables of interest
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine) %>% # select variables of interest
drop_na()
# Fix some data types
df$temp_low <- as.numeric(df$temp_low)
df$temp_high <- as.numeric(df$temp_high)
# Convert character columns to factors
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)],
as.factor)
# Set dependent variable name to y
names(df)[1] <- "y"
# Factorise dependent variable
df$y <- as.factor(df$y)
# Make sure there are no missings
if(any(is.na(df))) print("WARNING!! There are still incomplete samples present")
# Create preprocessing recipe for tuning and training
ml_rec <- recipe(y ~ ., data = df) %>%
#step_range(all_numeric()) %>% # Min-max normalisation
step_dummy(all_predictors() & where(is.factor)) %>% # Convert to dummy variables
themis::step_upsample(y)
# Create preprocessing recipe for testing/predicting
test_rec <- recipe(y ~ ., data = df) %>%
step_dummy(all_predictors() & where(is.factor))# Convert to dummy variables
# Set some naming to base the exported file names on
tune_name <- "_tuning_extra_variables"
train_name <- "_training_extra_variables"
plot_name <- "_plot_extra_variables"
# Set hyperparameter values to evaluate
trees_val <- c(10, 100,1000, 2000)
mtry_val <- c(1, 10, 20)
min_n_val <- c(0, 1, 2, 5, 10, 20, 40)
tree_depth_val <- c(1, 2, 5, 10, 20)
learn_rate_val <- c(1e-8, 1e-4, 1e-2, 1e-1, 0.5, 1)
loss_reduction_val <- c(1e-10, 1e-5, 0.1, 10)
# Tune XGBoost hyperparamters
tune_res_xg <- tune_xgboost(df, ml_rec, trees_val = trees_val,
mtry_val = mtry_val, min_n_val = min_n_val,
tree_depth_val = tree_depth_val,
learn_rate_val = learn_rate_val,
loss_reduction_val = loss_reduction_val, rep = 5,
seed = seed, parallel_comp = TRUE, verbose = TRUE,
k = 5, save = TRUE, entropy_grid = F,
save_name = paste("XGBoost", tune_name, sep = ""))
# Set evaluation metric to choose best hyper parameter values
metric = "mn_log_loss"
# Train XGBoost
model_xg <- train_xgboost(df, ml_rec, save = TRUE,
save_name = paste("XGBoost", train_name, sep = ""),
mtry = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$mtry,
trees = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$trees,
min_n = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$min_n,
tree_depth = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$tree_depth,
learn_rate = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$learn_rate,
loss_reduction = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$loss_reduction)
#=======================#
#                       #
#  Logistic Regression  #
#                       #
#=======================#
df_model3 <- juice(prep(ml_rec))
model3 <- lrm(y ~ ., data = df_model3,
x = TRUE, y = TRUE)
model3
#=======================#
#                       #
#     Visualisation     #
#                       #
#=======================#
proba_plot_xg <- plot_proba_truth(model_xg, df, test_rec, type = "violin",
"XGBoost - predicted probabilities",
save = TRUE,
save_name =
paste("XGBoost_predicted_probabilities_",
plot_name, sep = ""))
# Spider/radar chart
radar_chart <- plot_radar(list("XGBoost" = model_xg,
"model3" = model3),
test = list("XGBoost" = df,
"model3" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= T)
# Variable importance
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 4)
shapley_summary <- plot_shaply_summary(juice(prep(test_rec)), save = TRUE,
n_features = 4,
params = select_best(tune_res_xg$tune_res,
"mn_log_loss"),
save_name = paste("shapley_summary",
plot_name, sep = ""))
shapley_feature_imp <- ggplot(shapley_info, aes(x = variable, y = mean_value,
fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
ggsave(shapley_feature_imp, filename = paste("XGBoost_shapley_importances_",
plot_name, ".svg", sep = ""))
roc_model3 <- pROC::roc(df_model3$y, predict(model3, juice(prep(test_rec)),
type = "fitted"), ci = TRUE)
roc_model_xg <- pROC::roc(juice(prep(test_rec))$y,
predict_classprob.model_fit(model_xg, juice(prep(test_rec)))[[1]],
ci = TRUE)
ggroc(roc_model_xg, legacy.axes = TRUE) +
geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey",
linetype="dashed")
print(paste("XGBoost achieved an auc of", auc(roc_model_xg), "and LR an auc of",
auc(roc_model3), "and model3 and auc of", auc(r)))
radar_chart <- plot_radar(list("XGBoost" = model_xg,
"model3" = model3),
test = list("XGBoost" = df,
"model3" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= T)
radar_chart <- plot_radar(list("XGBoost" = model_xg),
test = list("XGBoost" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= T)
construct_decision_matrix(dp, model_xg)
construct_decision_matrix(dp, model_xg)
#' is also the day at which should be decided whether a bed should be emptied.
#' @param df altered data.frame used for training and testing the model which
#' contains all the sample used for testing but not all variables. Must be
#' supplied together with recipe
#' @param test_rec preprocessing recipe. Must be supplied together with df.
#'
#' @return
#' @export
#'
#' @examples
construct_decision_matrix <- function(dp, model, df = NULL, test_rec = NULL,
day = 7){
# If a df has been supplied, it is possible that dp contains different
# samples than the data the model will be tested on, therefore we here
# ensure that only samples present in the testing data are retained, and in
# an identical order
if(!is.null(df)){
dp <- dp[dp$Record.Id %in% row.names(df),]
df <- df[row.names(df) %in% dp$Record.Id,]
dp <- arrange(dp, Record.Id) # order samples
if(!all(dp$Record.Id == row.names(df)))
stop("samples in dp and df are not ordered identically")
}
## Afkappunten bepalen
if(class(model)[1] == "lrm"){
kans <- predict(model, type = "fitted")
} else {
kans <- 1 - predict_classprob.model_fit(model_xg, bake(prep(test_rec),
new_data = df))[[1]]
}
hist(kans)
afkap <- seq(25, 95, 2.5)
sens  <- rep(NA, length(afkap))
spec  <- rep(NA, length(afkap))
ppv   <- rep(NA, length(afkap))
npv   <- rep(NA, length(afkap))
abst  <- rep(NA, length(afkap))
misc  <- rep(NA, length(afkap))
ligd  <- rep(NA, length(afkap))
ligp  <- rep(NA, length(afkap))
# Create data.frame containing true and predicted classed
df_eval <- data.frame(factor(dp$event, levels = c(0,1), labels = c(0,1)), kans)
df_eval$pred <- factor(ifelse(kans < 0.5, 0, 1), levels = c(0,1))
names(df_eval) <- c("truth", "prob", "pred")
for (i in 1:length(afkap)){
df_eval$pred  <- factor(ifelse(kans*100 < afkap[i], 0, 1), levels = c(0,1))
sens[i] <- round(yardstick::sensitivity(df_eval, "truth", "pred")$.estimate, 2)
spec[i] <- round(yardstick::specificity(df_eval, "truth", "pred")$.estimate, 2)
ppv[i]  <- round(yardstick::ppv(df_eval, "truth", "pred")$.estimate, 2)
npv[i]  <- round(yardstick::npv(df_eval, "truth", "pred")$.estimate, 2)
abst[i] <- sum(df_eval$pred  == 1, na.rm = TRUE) # How many people are predicted to die
misc[i] <- table(df_eval$pred, df_eval$truth)[2, 1] # How many people are incorrectly predicted to die
ligd[i] <- sum(ifelse(dp$ICU_LoS[df_eval$pred == 1] - day < 1, 0, # How many bed days have been saved
dp$ICU_LoS[df_eval$pred == 1] - day), na.rm = TRUE) # How much percentage of total bed days has been saved
ligp[i] <- round(ligd[i]/sum(dp$ICU_LoS, na.rm = TRUE)*100, 1)
}
## Afkap in (%), testkenmerken, aantal abstineren (let op: aantal al voor 7 dagen
## van IC af, maar dat weten we bij aanvang niet), aantal onjuist abstineren,
## gespaarde ligdagen, gespaarde ligdagen (%).
decision_matrix <- data.frame(afkap, sens, spec, ppv, npv, abst, misc, ligd, ligp)
return(decision_matrix)
}
construct_decision_matrix(dp, model_xg)
construct_decision_matrix(df, model_xg)
construct_decision_matrix(test_rec, model_xg)
