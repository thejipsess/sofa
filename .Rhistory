data.frame(stringsAsFactors = TRUE) %>%
drop_na()
summary(df)
# Prepare date for XGBoost
# df <- dp %>%
#         select(event, day1, delta, age, gender) # select variables of interest
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine, ALAT, glucose_high) %>% # select variables of interest
as.data.frame(stringsAsFactors = TRUE) %>%
drop_na()
summary(df)
df$sepsis
is.factor(df$sepsis)
# Prepare date for XGBoost
# df <- dp %>%
#         select(event, day1, delta, age, gender) # select variables of interest
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine, ALAT, glucose_high) %>% # select variables of interest
drop_na()
# Convert character columns to factors
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)],
as.factor)
summary(df)
View(df)
dp$ALAT
# Prepare date for XGBoost
# df <- dp %>%
#         select(event, day1, delta, age, gender) # select variables of interest
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine) %>% # select variables of interest
drop_na()
summary(df)
# Convert character columns to factors
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)],
as.factor)
summary(df)
# Prepare date for XGBoost
# df <- dp %>%
#         select(event, day1, delta, age, gender) # select variables of interest
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine) %>% # select variables of interest
drop_na()
# Fix some data types
df$temp_low <- as.numeric(df$temp_low)
df$temp_high <- as.numeric(df$temp_high)
# Convert character columns to factors
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)],
as.factor)
summary(df)
# Set dependent variable name to y
names(df)[1] <- "y"
# Factorise dependent variable
df$y <- as.factor(df$y)
# Make sure there are no missings
if(any(is.na(df))) print("WARNING!! There are still incomplete samples present")
# Create preprocessing recipe for tuning and training
ml_rec <- recipe(y ~ ., data = df) %>%
#step_range(all_numeric()) %>% # Min-max normalisation
step_dummy(all_predictors() & where(is.factor)) %>% # Convert to dummy variables
themis::step_upsample(y)
# Create preprocessing recipe for testing/predicting
test_rec <- recipe(y ~ ., data = df) %>%
step_dummy(all_predictors() & where(is.factor))# Convert to dummy variables
# Set some naming to base the exported file names on
tune_name <- "_tuning_extra_variables"
train_name <- "_training_extra_variables"
plot_name <- "_plot_extra_variables"
# Train XGBoost
model_xg <- train_xgboost(df, ml_rec, save = TRUE,
save_name = paste("XGBoost", train_name, sep = ""),
mtry = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$mtry,
trees = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$trees,
min_n = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$min_n,
tree_depth = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$tree_depth,
learn_rate = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$learn_rate,
loss_reduction = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$loss_reduction)
proba_plot_xg <- plot_proba_truth(model_xg, df, test_rec, type = "violin",
"XGBoost - predicted probabilities",
save = TRUE,
save_name =
paste("XGBoost_predicted_probabilities_",
plot_name, sep = ""))
# Spider/radar chart
radar_chart <- plot_radar(list("XGBoost" = model_xg),
test = list("XGBoost" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
shapley_summary <- plot_shaply_summary(juice(prep(test_rec)), save = TRUE,
n_features = 4,
select_best(tune_res_xg$tune_res,
"mn_log_loss"),
save_name = paste("shapley_summary",
plot_name, sep = ""))
# Variable importance
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 4)
shapley_info
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine) %>% # select variables of interest
drop_na()
# Fix some data types
df$temp_low <- as.numeric(df$temp_low)
df$temp_high <- as.numeric(df$temp_high)
# Convert character columns to factors
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)],
as.factor)
# Set dependent variable name to y
names(df)[1] <- "y"
# Factorise dependent variable
df$y <- as.factor(df$y)
# Make sure there are no missings
if(any(is.na(df))) print("WARNING!! There are still incomplete samples present")
# Create preprocessing recipe for tuning and training
ml_rec <- recipe(y ~ ., data = df) %>%
#step_range(all_numeric()) %>% # Min-max normalisation
step_dummy(all_predictors() & where(is.factor)) %>% # Convert to dummy variables
themis::step_upsample(y)
# Create preprocessing recipe for testing/predicting
test_rec <- recipe(y ~ ., data = df) %>%
step_dummy(all_predictors() & where(is.factor))# Convert to dummy variables
# Set some naming to base the exported file names on
tune_name <- "_tuning_extra_variables"
train_name <- "_training_extra_variables"
plot_name <- "_plot_extra_variables"
# Set hyperparameter values to evaluate
trees_val <- c(10, 100,1000, 2000)
mtry_val <- c(1, 10, 20)
min_n_val <- c(0, 1, 2, 5, 10, 20, 40)
tree_depth_val <- c(1, 2, 5, 10, 20)
learn_rate_val <- c(1e-8, 1e-4, 1e-2, 1e-1, 0.5, 1)
loss_reduction_val <- c(1e-10, 1e-5, 0.1, 10)
# Tune XGBoost hyperparamters
tune_res_xg <- tune_xgboost(df, ml_rec, trees_val = trees_val,
mtry_val = mtry_val, min_n_val = min_n_val,
tree_depth_val = tree_depth_val,
learn_rate_val = learn_rate_val,
loss_reduction_val = loss_reduction_val, rep = 5,
seed = seed, parallel_comp = TRUE, verbose = TRUE,
k = 5, save = TRUE, entropy_grid = TRUE,
save_name = paste("XGBoost", tune_name, sep = ""))
# Set evaluation metric to choose best hyper parameter values
metric = "mn_log_loss"
# Train XGBoost
model_xg <- train_xgboost(df, ml_rec, save = TRUE,
save_name = paste("XGBoost", train_name, sep = ""),
mtry = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$mtry,
trees = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$trees,
min_n = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$min_n,
tree_depth = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$tree_depth,
learn_rate = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$learn_rate,
loss_reduction = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$loss_reduction)
#=======================#
#                       #
#  Logistic Regression  #
#                       #
#=======================#
df_model3 <- juice(prep(ml_rec))
model3 <- lrm(y ~ ., data = df_model3,
x = TRUE, y = TRUE)
model3
#=======================#
#                       #
#     Visualisation     #
#                       #
#=======================#
proba_plot_xg <- plot_proba_truth(model_xg, df, test_rec, type = "violin",
"XGBoost - predicted probabilities",
save = TRUE,
save_name =
paste("XGBoost_predicted_probabilities_",
plot_name, sep = ""))
# Spider/radar chart
radar_chart <- plot_radar(list("XGBoost" = model_xg,
"model3" = model3),
test = list("XGBoost" = df,
"model3" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
# Variable importance
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 4)
shapley_summary <- plot_shaply_summary(juice(prep(test_rec)), save = TRUE,
n_features = 4,
select_best(tune_res_xg$tune_res,
"mn_log_loss"),
save_name = paste("shapley_summary",
plot_name, sep = ""))
roc_model3 <- pROC::roc(df_model3$y, predict(model3, juice(prep(test_rec)),
type = "fitted"), ci = TRUE)
roc_model_xg <- pROC::roc(juice(prep(test_rec))$y,
predict_classprob.model_fit(model_xg, juice(prep(test_rec)))[[1]],
ci = TRUE)
ggroc(roc_model_xg, legacy.axes = TRUE) +
geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey",
linetype="dashed")
print(paste("XGBoost achieved an auc of", auc(roc_model_xg), "and LR an auc of",
auc(roc_model3), "and model3 and auc of", auc(r)))
radar_chart <- plot_radar(list("XGBoost" = model_xg,
"model3" = model3),
test = list("XGBoost" = df,
"model3" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
radar_chart <- plot_radar(list("XGBoost" = model_xg),
test = list("XGBoost" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
plot_name <- "_plot_extra_variables_entropy"
auc(roc_model3)
auc(roc_model_xg)
proba_plot_xg <- plot_proba_truth(model_xg, df, test_rec, type = "violin",
"XGBoost - predicted probabilities",
save = TRUE,
save_name =
paste("XGBoost_predicted_probabilities_",
plot_name, sep = ""))
# Spider/radar chart
radar_chart <- plot_radar(list("XGBoost" = model_xg,
"model3" = model3),
test = list("XGBoost" = df,
"model3" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
radar_chart <- plot_radar(list("XGBoost" = model_xg),
test = list("XGBoost" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
# Variable importance
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 4)
shapley_summary <- plot_shaply_summary(juice(prep(test_rec)), save = TRUE,
n_features = 4,
select_best(tune_res_xg$tune_res,
"mn_log_loss"),
save_name = paste("shapley_summary",
plot_name, sep = ""))
df <- dp %>%
select(event, day1, delta, age, gender, BMI, sepsis,
systolicBP_high, temp_high_admission, temp_high, systolicBP_high,
systolicBP_high_admission, systolicBP_low_admission, systolicBP_low,
ventilation_admission, APACHE_II, temp_low_admission,
temp_low, bilirubine) %>% # select variables of interest
drop_na()
# Fix some data types
df$temp_low <- as.numeric(df$temp_low)
df$temp_high <- as.numeric(df$temp_high)
# Convert character columns to factors
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)],
as.factor)
# Set dependent variable name to y
names(df)[1] <- "y"
# Factorise dependent variable
df$y <- as.factor(df$y)
# Make sure there are no missings
if(any(is.na(df))) print("WARNING!! There are still incomplete samples present")
# Create preprocessing recipe for tuning and training
ml_rec <- recipe(y ~ ., data = df) %>%
#step_range(all_numeric()) %>% # Min-max normalisation
step_dummy(all_predictors() & where(is.factor)) %>% # Convert to dummy variables
themis::step_upsample(y)
# Create preprocessing recipe for testing/predicting
test_rec <- recipe(y ~ ., data = df) %>%
step_dummy(all_predictors() & where(is.factor))# Convert to dummy variables
# Set some naming to base the exported file names on
tune_name <- "_tuning_extra_variables"
train_name <- "_training_extra_variables"
plot_name <- "_plot_extra_variables"
# Set hyperparameter values to evaluate
trees_val <- c(10, 100,1000, 2000)
mtry_val <- c(1, 10, 20)
min_n_val <- c(0, 1, 2, 5, 10, 20, 40)
tree_depth_val <- c(1, 2, 5, 10, 20)
learn_rate_val <- c(1e-8, 1e-4, 1e-2, 1e-1, 0.5, 1)
loss_reduction_val <- c(1e-10, 1e-5, 0.1, 10)
# Tune XGBoost hyperparamters
tune_res_xg <- tune_xgboost(df, ml_rec, trees_val = trees_val,
mtry_val = mtry_val, min_n_val = min_n_val,
tree_depth_val = tree_depth_val,
learn_rate_val = learn_rate_val,
loss_reduction_val = loss_reduction_val, rep = 5,
seed = seed, parallel_comp = TRUE, verbose = TRUE,
k = 5, save = TRUE, entropy_grid = F,
save_name = paste("XGBoost", tune_name, sep = ""))
# Set evaluation metric to choose best hyper parameter values
metric = "mn_log_loss"
# Train XGBoost
model_xg <- train_xgboost(df, ml_rec, save = TRUE,
save_name = paste("XGBoost", train_name, sep = ""),
mtry = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$mtry,
trees = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$trees,
min_n = show_best(tune_res_xg$tune_res,
metric = metric, n=1)$min_n,
tree_depth = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$tree_depth,
learn_rate = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$learn_rate,
loss_reduction = show_best(tune_res_xg$tune_res,
metric = metric,
n=1)$loss_reduction)
#=======================#
#                       #
#  Logistic Regression  #
#                       #
#=======================#
df_model3 <- juice(prep(ml_rec))
model3 <- lrm(y ~ ., data = df_model3,
x = TRUE, y = TRUE)
model3
#=======================#
#                       #
#     Visualisation     #
#                       #
#=======================#
proba_plot_xg <- plot_proba_truth(model_xg, df, test_rec, type = "violin",
"XGBoost - predicted probabilities",
save = TRUE,
save_name =
paste("XGBoost_predicted_probabilities_",
plot_name, sep = ""))
# Spider/radar chart
radar_chart <- plot_radar(list("XGBoost" = model_xg,
"model3" = model3),
test = list("XGBoost" = df,
"model3" = df),
dep_var = df$y, vlcex = 1.5,
rec = prep(test_rec),
title = "", save = T,legend_offset = c(0.04, -0.05),
save_name = paste("radar_chart", plot_name, sep = ""),
alpha = 0.2, chance= FALSE)
# Variable importance
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 4)
shapley_summary <- plot_shaply_summary(juice(prep(test_rec)), save = TRUE,
n_features = 4,
select_best(tune_res_xg$tune_res,
"mn_log_loss"),
save_name = paste("shapley_summary",
plot_name, sep = ""))
roc_model3 <- pROC::roc(df_model3$y, predict(model3, juice(prep(test_rec)),
type = "fitted"), ci = TRUE)
roc_model_xg <- pROC::roc(juice(prep(test_rec))$y,
predict_classprob.model_fit(model_xg, juice(prep(test_rec)))[[1]],
ci = TRUE)
ggroc(roc_model_xg, legacy.axes = TRUE) +
geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey",
linetype="dashed")
print(paste("XGBoost achieved an auc of", auc(roc_model_xg), "and LR an auc of",
auc(roc_model3), "and model3 and auc of", auc(r)))
shapley_summary <- plot_shaply_summary(juice(prep(test_rec)), save = TRUE,
n_features = 10,
select_best(tune_res_xg$tune_res,
"mn_log_loss"),
save_name = paste("shapley_summary",
plot_name, sep = ""))
?shap.plot.dependence
?shap.importance
shapley_info$variable
shapley_info$mean_value
?shap.prep
shap1 <- shap.prep(model_xg, juice(prep(test_rec)))
shap1 <- shap.prep(model_xg, X_train = juice(prep(test_rec)))
shap1 <- shap.prep(xgb_model = model_xg, X_train = juice(prep(test_rec)))
shap1 <- shap.prep(X_train = juice(prep(test_rec)))
auc(roc_model_xg)
shapley_info
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 10)
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 12)
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 13)
shapley_info <- get_shaply_info(juice(prep(test_rec)), simplify = TRUE,
select_best(tune_res_xg$tune_res, "mn_log_loss"),
n_features = 17)
shapley_info
plot(shapley_info)
ggplot(shapley_info, aes(x = variable, y = mean_value)) +
geom_bar()
ggplot(shapley_info, aes(y = mean_value)) +
geom_bar()
ggplot(shapley_info, aes(x = variable)) +
geom_bar()
ggplot(shapley_info, aes(x = variable, y = mean_value)) +
geom_bar(stat = "identity")
ggplot(shapley_info, aes(x = variable, y = mean_value)) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(shapley_info, aes(x = variable, y = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + xlab("Mean variable importance") + ylab("")
ggplot(shapley_info, aes(x = variable, y = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean variable importance") + xlab("")
ggplot(shapley_info, aes(x = variable, y = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")
ggplot(shapley_info, aes(x = variable, y = mean_value, color = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0.2)) + scale_y_continuous(expand = c(0, 0.2))
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0.2))
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0.2, 0))
ggplot(shapley_info, aes(x = variable, y = mean_value, fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
shapley_feature_imp <- ggplot(shapley_info, aes(x = variable, y = mean_value,
fill = mean_value)) +
geom_bar(stat = "identity") +
coord_flip() + ylab("Mean absolute variable importance") + xlab("")+
scale_x_discrete(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
ggsave(shapley_feature_imp, filename = paste("XGBoost_shapley_importances_",
plot_name, sep = ""))
ggsave(shapley_feature_imp, filename = paste("XGBoost_shapley_importances_",
plot_name, sep = ""), device =".png")
ggsave(shapley_feature_imp, filename = paste("XGBoost_shapley_importances_",
plot_name, ".svg" sep = ""))
ggsave(shapley_feature_imp, filename = paste("XGBoost_shapley_importances_",
plot_name, ".svg", sep = ""))
ggsave(shapley_feature_imp, filename = paste("XGBoost_shapley_importances_",
plot_name, ".png", sep = ""))
